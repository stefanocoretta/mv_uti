{
  "hash": "39bb45cb992b83d4e3f2664d7e34db66",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multivariate analyses of tongue contours from ultrasound tongue imaging. Draft v0.7\"\nauthors:\n  - name: Stefano Coretta\n    affiliation: University of Edinburgh\n    roles: [conceptualisation, data curation, methodology, software, supervision, validation, visualisation, writing, editing]\n    corresponding: true\n    orcid: 0000-0001-9627-5532\n    email: s.coretta@ed.ac.uk\n  - name: Georges Sakr\n    affiliation: University of Edinburgh\n    orcid: 0000-0003-3813-2669\n    roles: [data curation, methodology, writing, editing]\nbibliography: references.bib\ncitation:\n  url: \"https://stefanocoretta.github.io/mv_uti/\"\ngoogle-scholar: true\ndate: \"2025-04-23\"\n---\n\n\n\n\n# Introduction\n\n\n\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ntheme_set(theme_light())\nlibrary(knitr)\n```\n:::\n\n\n\n\n::: callout-warning\nThis is a \"living\" draft, meaning it is work in progress. While the code is fully functional and usable, we will be updating the textual explanation and might make minor changes to the code to improve clarity. Please, if using in research, cite the version you have consulted. The version of the draft is given in the title as \"Draft vX.X\" where \"X\" are incremental digits. See citation recommendation at the bottom of the document.\n:::\n\nUltrasound Tongue Imaging (UTI) is a non-invasive technique that allows researchers to image the shape of the tongue during speech at medium temporal resolution [30-100 frames per second, @epstein2005; @stone2005]. Typically, the midsagittal contour of the tongue is imaged, although 3D systems exist [@lulich2018]. Recent developments in machine learning assisted image processing has enabled faster tracking of estimated points on the tongue contour [@wrench2022].\n\n@wrench2022 have trained a DeepLabCut (DLC) model to estimate and track specific flesh points on the tongue contour and anatomical landmarks as captured by UTI. The model estimates 11 \"knots\" from the vallecula to the tongue tip, plus three muscular-skeletal knots, the hyoid bone, the mandible base and the mental spine where the short tendon attaches (see @fig-knots for a schematic illustration of the position of the tracked knots). An advantage of DLC-tracked data over the traditional fan-line coordinate system is that (in theory) specific (moving) flesh points are tracked rather than simply the intersection of the tongue contour with fixed radii from the fan-line system. This makes DLC-tracked data resemble data obtained with electromagnetic articulography (EMA). The downside is that the tongue contour is represented by 11 freely moving points, which can move in any direction in the midsagittal two-dimensional space captured by UTI.\n\n![Schematic representation of the knots tracked by DeepLabCut. CC-BY Wrench and Balch-Tomes [@wrench2024].](img/sensors-22-01133-g002.jpg){#fig-knots fig-align=\"center\"}\n\nClassical ways to analyse tongue contour data obtained from a fan-line system, like SS-ANOVA [@davidson2006; @chen2011] and Generalised Additive Models using polar coordinates [@coretta2018c; @coretta2019g], are not appropriate with DLC-tracked data, due to the tongue contour \"curling\" onto itself along the root. This is illustrated in @fig-curl: the plot shows the DLC-tracked points (in black) of the data from a Polish speaker and the traced tongue contours based on the points (see @sec-gam-vc-coart for details on the data). The contours clearly curl onto themselves along the root (on the left of the contour). The red smooths represent a LOESS smooth, calculated for Y along X. This approach clearly miscalculates the smooth for the back half of the tongue, simply because there are two Y values for the same X value, and the procedure, in that case, returns something like an average of the two values. Generalised Additive Models (introduced in the following section) work on the same principle and hence would produce the same type of error. Using polar coordinates would not solve the problem: while a fan-line system lends itself easily to using polar coordinates (since the origin of the probe can be used to approximate the origin of the coordinate system), this cannot be done with DLC data because there is, in reality, no single origin in the actual tongue anatomy from which vectors of displacement radiate, that would work for all tracked points.\n\n\n\n\n::: {#cell-fig-curl .cell}\n\n```{.r .cell-code .hidden}\ndlc_voff_f <- readRDS(\"data/coretta2018/dlc_voff_f.rds\")\n\ndlc_voff_f |> \n  filter(speaker == \"pl04\") |> \n  ggplot(aes(X, Y)) +\n  geom_point(alpha = 0.25) +\n  geom_path(aes(group = frame_id), alpha = 0.25) +\n  geom_smooth(colour = \"red\", method = \"loess\", formula = \"y ~ x\") +\n  coord_fixed() +\n  facet_grid(vowel ~ c2) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![Illustrating tongue contours curling up along the root. The estimated smooths in red fail to capture the curl.](index_files/figure-html/fig-curl-1.png){#fig-curl width=672}\n:::\n:::\n\n\n\n\nIn this tutorial, we introduce two alternative methods to analyse DLC-tracked tongue contour data: Multivariate Generalised Additive Models (@sec-gam) and Multivariate Functional Principal Component Analysis (@sec-fpca). We will present the pros and cons of each method in @sec-procons, but to summarise we are inclined to recommend Multivariate Functional Principal Component Analysis over Multivariate Generalised Additive Models due to the substantial computational overhead and reduced practical utility of the latter over the former.\n\n# Multivariate Generalised Additive Models {#sec-gam}\n\nGeneralised Additive Models (GAMs) are an extension of generalised models that allow flexible modelling of non-linear effects [@hastie1986; @wood2006]. GAMs are built upon smoothing splines functions, the components of which are multiplied by estimated coefficients to reconstruct an arbitrary time-changing curve. For a thorough introduction to GAMs we refer the reader to [@soskuthy2017a; @soskuthy2021; @pedersen2019; @wieling2018]. Multivariate Generalised Additive Models (MGAMs) are GAMs with more than one outcome variable.\n\nAs mentioned in the Introduction, the data tracked by DeepLabCut consists of the position on the horizontal (*x*) and vertical (*y*) axes of fourteen knots. In this tutorial, we will focus on modelling the tongue contour based on the 11 knots from the vallecula to the tongue tip. @fig-tongue illustrates the reconstructed tongue contour on the basis of the 11 knots: the shown tongue is from the offset of a vowel \\[o\\] followed by \\[t\\], uttered by a Polish speaker (see @sec-gam-vc-coart).\n\n\n\n\n::: {#cell-fig-tongue .cell}\n\n```{.r .cell-code .hidden}\ndlc_voff_f |> \n  filter(speaker == \"pl04\", frame_id == 432) |> \n  ggplot(aes(X, Y, group = frame_id)) +\n  geom_point() +\n  geom_path() +\n  coord_fixed() +\n  labs(x = \"X (mm)\", y = \"Y (mm)\")\n```\n\n::: {.cell-output-display}\n![The eleven knots on the tongue contour taken from the offset of [o] followed by [t] (Polish speaker PL04, tongue tip to the right).](index_files/figure-html/fig-tongue-1.png){#fig-tongue width=672}\n:::\n:::\n\n\n\n\nThe same data is shown in @fig-tongue-xy, but in a different format. Instead of a Cartesian coordinate system of X and Y values, the plot has knot number on the *x*-axis and X/Y coordinates on the *y*-axis. The X/Y coordinates thus form \"trajectories\" along the knots. These X/Y trajectories are the ones that can be modelled using MGAMs and Multiple Functional Principal Component Analysis (MFPCA): in both cases, the X/Y trajectories are modelled as two variables changing along knot number. In this section, we will illustrate GAMs applied to the X/Y trajectories along the knots and how we can reconstruct the tongue contour from the modelled trajectories. We will use data from two case studies of coarticulation: vowel consonant (VC) coarticulation based on C place in Italian and Polish, and consonantal articulation of plain vs emphatic consonants in Lebanese Arabic.\n\n\n\n\n::: {#cell-fig-tongue-xy .cell}\n\n```{.r .cell-code .hidden}\ndlc_voff_f |> \n  filter(speaker == \"pl04\", frame_id == 432) |> \n  dplyr::select(knot, X, Y) |> \n  pivot_longer(c(X,Y)) |> \n  ggplot(aes(knot + 1, value)) +\n  geom_point() +\n  geom_path() +\n  facet_grid(rows = vars(name)) +\n  scale_x_continuous(breaks = 1:11) +\n  labs(x = \"Knot\", y = \"Position (mm)\")\n```\n\n::: {.cell-output-display}\n![The horizontal and vertical positions of the elevel knots (same data as @fig-tongue).](index_files/figure-html/fig-tongue-xy-1.png){#fig-tongue-xy width=672}\n:::\n:::\n\n\n\n\n## VC coarticulation {#sec-gam-vc-coart}\n\nThe data of the first case study, @coretta2018f, comes from @coretta2020b and have been discussed in @coretta2020 (the analysis concerned the position of the tongue root during the duration of vowels followed by voiceless or voiced stops; in this paper we focus on tongue contours at the vowel offset). The materials are /pVCV/ words embedded in a frame sentence (*Dico X lentamente* 'I say X slowly' in Italian and *Mówię X teraz* 'I say X now' in Polish). In the /pVCV/ words, C was /t, d, k, ɡ/ and V was /a, o, u/ (in each word, the two vowels where identical, so for example *pata, poto, putu*). The data analysed here is from 9 speakers of Italian and 6 speakers of Polish (other speakers were not included due to the difficulty in processing their data with DeepLabCut).\n\nUltrasound tongue imaging was obtained with the set up by Articulate Assistant Advanced™ [AAA, @ltd2011]. Spline data was extracted using a custom DeepLabCut (DLC) model developed by @wrench2022. When exporting from AAA™, the data was rotated based on the bite plane, obtained with the imaging of a bite plate [@scobbie2011], so that the bite plane is horizontal: this allows for a common coordinate system where vertical and horizontal movement are comparable across speakers. Once the DLC data was imported in R, we manually removed tracking errors and we calculated *z*-scores within each speaker (the difference between the value and the mean, divided by the standard deviation). These steps are documented in the paper's notebook [Prepare data](notebooks/01_prepare_data.qmd).\n\nThe following code chunk reads the filtered data. A sample of the data is shown in @tbl-dlc-voff. @fig-voff shows the tongue contours for each individual speaker. It is possible to notice clusters of different contours, related to each of the vowels /a, o, u/. @fig-pl04 zooms in on PL04 (Polish): the contours of each vowel are coloured separately, and two panels separate tongue contours taken at the offset of vowels followed by coronal (/t, d/) and velar stops (/k, ɡ/). Crucially, the variation in tongue shape at vowel offset (or closure onset) across vowels contexts is higher in the coronal than in the velar contexts. This is not surprising, giving the greater involvement of the tongue body and dorsum (the relevant articulators of vowel production) in velar than in coronal stops.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndlc_voff_f <- readRDS(\"data/coretta2018/dlc_voff_f.rds\")\n```\n:::\n\n::: {#tbl-dlc-voff .cell tbl-cap='A sample of the VC coarticulation data from @coretta2018f.'}\n\n```{.r .cell-code .hidden}\nhead(dlc_voff_f |> select(speaker, word, X, Y, knot, knot_label)) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|speaker |word |        X|        Y| knot|knot_label |\n|:-------|:----|--------:|--------:|----:|:----------|\n|it01    |pugu | -55.2105| -44.1224|    0|Vallecula  |\n|it01    |pugu | -60.6994| -31.3486|    1|Root_1     |\n|it01    |pugu | -65.1434| -17.7311|    2|Root_2     |\n|it01    |pugu | -63.6757|  -4.2022|    3|Body_1     |\n|it01    |pugu | -57.2505|   7.8483|    4|Body_2     |\n|it01    |pugu | -44.9086|  13.3162|    5|Dorsum_1   |\n\n\n:::\n:::\n\n::: {#cell-fig-voff .cell}\n\n```{.r .cell-code .hidden}\ndlc_voff_f |> \n  ggplot(aes(X_z, Y_z, group = frame_id)) +\n  geom_path(alpha = 0.25) +\n  coord_fixed() +\n  facet_wrap(vars(speaker), ncol = 5)\n```\n\n::: {.cell-output-display}\n![Tongue contours of 9 Italian speakers and6 Polish speakers, taken from the offset of the first vowel in /pCVCV/ target words.](index_files/figure-html/fig-voff-1.png){#fig-voff width=672}\n:::\n:::\n\n::: {#cell-fig-pl04 .cell}\n\n```{.r .cell-code .hidden}\ndlc_voff_f |> \n  filter(speaker == \"pl04\") |> \n  ggplot(aes(X_z, Y_z, group = frame_id, colour = vowel)) +\n  geom_path(alpha = 0.5) +\n  coord_fixed() +\n  facet_grid(cols = vars(c2_place)) +\n  labs(x = \"X (z-scores)\", \"Y (z-scores)\")\n```\n\n::: {.cell-output-display}\n![Tongue contours of PL04 (Polish) taken from the offset of vowels followed by coronal or velar stops. Tip is on the right.](index_files/figure-html/fig-pl04-1.png){#fig-pl04 width=672}\n:::\n:::\n\n\n\n\nWe can now run a multivariate GAM to model the tongue contours. A multivariate GAM can be fitted by providing model formulae for each outcome variable (in our case, `X_z` and `Y_z`) in a list. For example `list(y ~ s(x), w ~ s(x))` would instruct `mgcv::gam()` to fit a bivariate GAM with the two outcome variables `y` and `w`. The required family is `mvn` for \"multivariate normal\": `mvn(d = 2)` indicates a bivariate family (a multivariate family with two dimensions, i.e. two outcome variables). In the model below, we are fitting a multivariate GAM to the *z*-scored X and Y coordinates. For both outcome variables, we include a smooth over knot (`s(knot, ...)`) with a `by` variable `vow_place_lang`: this variable is built from an interaction of vowel, place and language.[^1] We set `k` to 5: this will usually be sufficient for X/Y coordinates of tongue contours, since they are by nature not very \"wiggly\" (which would require a higher `k`). We also include a factor smooth over knot for speaker (the equivalent of a non-linear random effect) with `s(knot, speaker, ...)`: since language is a between-speaker variable, we use `vow_place` as the `by` variable (`vow_place` is the interaction of vowel and place).\n\n[^1]: Note that interactions between categorical variables in the classical sense are not possible in GAMs. Instead, one can approximate interactions by creating an \"interaction variable\", which is simply a variable where the values of the interacting variables are pasted together.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mgcv)\n\nvoff_gam <- gam(\n  list(\n    X_z ~ vow_place_lang +\n      s(knot, by = vow_place_lang, k = 5) +\n      s(knot, speaker, by = vow_place, bs = \"fs\", m = 1),\n    Y_z ~ vow_place_lang +\n      s(knot, by = vow_place_lang, k = 5) +\n      s(knot, speaker, by = vow_place, bs = \"fs\", m = 1)\n  ),\n  data = dlc_voff_f,\n  family = mvn(d = 2)\n)\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nlibrary(mgcv)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: nlme\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'nlme'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    collapse\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThis is mgcv 1.9-3. For overview type 'help(\"mgcv-package\")'.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nfi <- \"data/cache/voff_gam.rds\"\n\n# Manual cache\nif (file.exists(fi)) {\n  voff_gam <- readRDS(fi)\n} else {\n  voff_gam <- gam(\n    list(\n      X_z ~ vow_place_lang +\n        s(knot, by = vow_place_lang, k = 5) +\n        s(knot, speaker, by = vow_place, bs = \"fs\", m = 1),\n      Y_z ~ vow_place_lang +\n        s(knot, by = vow_place_lang, k = 5) +\n        s(knot, speaker, by = vow_place, bs = \"fs\", m = 1)\n    ),\n    data = dlc_voff_f,\n    family = mvn(d = 2)\n  )\n  \n  saveRDS(voff_gam, fi)\n}\n```\n:::\n\n\n\n\nThe model summary is not particular insightful. What we are normally interested in is the reconstructed tongue contours and in which locations they are similar of different across conditions. To the best of our knowledge, there isn't a straightforward way to compute sensible measures of comparison, given the multidimensional nature of the model (i.e., only one or the other outcome can be inspected at a time; moreover, difference smooths, like in @soskuthy2017a and @wieling2018, represent the difference of the *sum* of the outcome variables, rather than each outcome separately, Michele Gubian pers. comm.) We thus recommend to plot the predicted tongue contours and base any further inference on impressionistic observations on such predicted contours. Alas, there is also no straightforward way to plot predicted tongue contours, but to extract the predictions following a step-by-step procedure, like the one illustrated in the following paragraphs.\n\nFirst off, one has to create a grid of predictor values to obtain predictions for. We do this with `expand_grid()` in the following code chunk. We start with unique values of `speaker`, `vow_place` and `knot` (rather than just using integers for the knots, we predict along increments of 0.1 from 0 to 10 for a more refined tongue contour). We then create the required column `vow_place_lang` by appending the language name based on the speaker ID. Note that all variables included as predictors in the model must be included in the prediction grid.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a grid of values to predict for\nframe_voff <- expand_grid(\n  # All the speakers\n  speaker = unique(dlc_voff_f$speaker),\n  # All vowel/place combinations\n  vow_place = unique(dlc_voff_f$vow_place),\n  # Knots from 0 to 10 by increments of 0.1\n  # This gives us greater resolution along the tongue contour than just using 10 knots\n  knot = seq(0, 10, by = 0.1)\n) |> \n  mutate(\n    vow_place_lang = case_when(\n      str_detect(speaker, \"it\") ~ paste0(vow_place, \".Italian\"),\n      str_detect(speaker, \"pl\") ~ paste0(vow_place, \".Polish\")\n    )\n  )\n```\n:::\n\n\n\n\nWith the prediction grid `frame_voff` we can now extract predictions from the model `voff_gam` with `predict()`. This function requires the GAM model object (`voff_gam`) and the prediction grid (`frame_off`). We also obtain the standard error of the prediction which we will use to calculate Confidence Intervals in the next step. Since we have used factor smooths for speaker, we now have to manually exclude these smooths from the prediction to obtain a \"population\" level prediction. We do this by listing the smooths to be removed in `excl`: note that the smooths must be named as they are in the summary of the model, so always check the summary to ensure you list all of the factor smooths. Finally, we rename the columns with the name of the outcome variables.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# List of factor smooths, to be excluded from prediction\nexcl <- c(\n  \"s(knot,speaker):vow_placea.coronal\",\n  \"s(knot,speaker):vow_placeo.coronal\",\n  \"s(knot,speaker):vow_placeu.coronal\",\n  \"s(knot,speaker):vow_placea.velar\",\n  \"s(knot,speaker):vow_placeo.velar\",\n  \"s(knot,speaker):vow_placeu.velar\",\n  \"s.1(knot,speaker):vow_placea.coronal\",\n  \"s.1(knot,speaker):vow_placeo.coronal\",\n  \"s.1(knot,speaker):vow_placeu.coronal\",\n  \"s.1(knot,speaker):vow_placea.velar\",\n  \"s.1(knot,speaker):vow_placeo.velar\",\n  \"s.1(knot,speaker):vow_placeu.velar\"\n)\n\n# Get prediction from model voff_gam\nvoff_gam_p <- predict(voff_gam, frame_voff, se.fit = TRUE, exclude = excl) |>\n  as.data.frame() |>\n  as_tibble()\n\n# Rename columns\ncolnames(voff_gam_p) <- c(\"X\", \"Y\", \"X_se\", \"Y_se\")\n```\n:::\n\n\n\n\nNow we have to join the prediction in `voff_gam_p` with the prediction frame, so that we have all the predictor values in the same data frame. We do so here with `bind_cols()` from the dplyr package. Note that `voff_gam_p` contains predictions for each level of the factor smooths, despite these being excluded from prediction. If you inspect the predictions for different speakers, you will find that they are the same for the same levels of `vow_place_lang`: this is because the effects of the factor smooths were removed, so `speaker` has no effect on the predicted values. This means that you can pick any Italian and Polish speaker in the predicted data frame. We do so by filtering with `filter(speaker %in% c(\"it01\", \"pl02\"))`, but any other speaker would lead to the same output. We also calculate the lower and upper limits of 95% Confidence intervals (CI) for each coordinate. Note that you should interpret these CI with a grain of salt, because they are not truly multivariate, but rather represent the CI on each coordinate axis independently.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvoff_gam_p <- bind_cols(frame_voff, voff_gam_p) |> \n  # pick any Italian and Polish speaker, random effects have been removed\n  filter(speaker %in% c(\"it01\", \"pl02\")) |> \n  # Calculate 95% CIs of X and Y\n  mutate(\n    X_lo = X - (1.96 * X_se),\n    X_hi = X + (1.96 * X_se),\n    Y_lo = Y - (1.96 * Y_se),\n    Y_hi = Y + (1.96 * Y_se)\n  ) |> \n  # Separate column into individual variables, for plotting later\n  separate(vow_place_lang, c(\"vowel\", \"place\", \"language\"))\n```\n:::\n\n\n\n\n@fig-voff-pred and @fig-voff-ci show the predicted tongue contours based on the `voff_gam` model, without and with 95% CIs respectively. As mentioned earlier, there isn't a straightforward way to obtain any statistical measure of the difference between the contours on the multivariate plane, so we must be content with the figure.\n\n\n\n\n::: {#cell-fig-voff-pred .cell}\n\n```{.r .cell-code .hidden}\nvoff_gam_p |> \n  ggplot(aes(X, Y, colour = vowel)) +\n  geom_point(alpha = 0.5) +\n  facet_grid(cols = vars(place), rows = vars(language)) +\n  coord_fixed() +\n  labs(\n    x = \"X (z-scores)\",\n    y = \"Y (z-scores)\"\n  )\n```\n\n::: {.cell-output-display}\n![Predicted tongue contours based on a multivariate GAM. Uncertainty not shown.](index_files/figure-html/fig-voff-pred-1.png){#fig-voff-pred width=672}\n:::\n:::\n\n::: {#cell-fig-voff-ci .cell}\n\n```{.r .cell-code .hidden}\nvoff_gam_p |> \n  group_by(place, vowel, language) |> \n  mutate(\n    Y_lo = ifelse(Y_lo > min(Y), Y_lo, NA),\n    X_hi = ifelse(X_hi < max(X), X_hi, NA),\n  ) |> \n  ggplot(aes(X, Y, colour = vowel)) +\n  geom_errorbarh(aes(xmin = X_lo, xmax = X_hi), alpha = 0.5) +\n  geom_errorbar(aes(ymin = Y_lo, ymax = Y_hi), alpha = 0.5) +\n  geom_point(size = 1, alpha = 0.75) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  coord_fixed() +\n  facet_grid(cols = vars(place), rows = vars(language)) +\n  theme_light() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: Removed 57 rows containing missing values or values outside the scale range\n(`geom_errorbarh()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Predicted tongue contours based on a multivariate GAM, with 95% Confidence Intervals.](index_files/figure-html/fig-voff-ci-1.png){#fig-voff-ci width=672}\n:::\n:::\n\n\n\n\n## Emphaticness {#sec-gam-emphaticness}\n\nThe second case study is about consonant \"emphaticness\" in Lebanese Arabic. The data is from @sakr2025. [XXX TODO GEORGE description of the data, including a brief explanation of the LebAr context]. The second case study is about consonant \"emphaticness\" in Lebanese Arabic.\n\nLebanese Arabic is a variety of Arabic primarily spoken in Lebanon, where it is in constant contact with a number of Indo-European languages [primarily English and French, as vectors of education and business, see eg. @shaaban1999], as well as the written standard form of Arabic known as Modern Standard Arabic (MSA). The relationship between Lebanese Arabic (LA) and MSA in Lebanon is one of diglossia [see eg. @lian2022], where LA is spoken in most contexts, but not written, and MSA is the written variety, and therefore also primarily used for legal and official purposes.\n\nEmphasis is a phonologically contrastive feature of Semitic languages. In most varieties of Arabic, it is usually reported to be realised as pharyngealisation [@sakr2023; @al-tamimi2017; @zeroual2011; @watson2002] with some variation depending on phonological context [@sakr2025; @al-tamimi2011] or on sociolinguistic factors [@khattab2006]. Older sources instead report the secondary place of articulation as being the velum [see eg. @obrecht1968; @nasr1959] or the uvula [see eg. @bin-muqbil2006; @zawaydeh1999; @ghazeli1977]. Whatever the specifics of this secondary place of articulation, the literature [see among others @sullivan2017; @el-khaissi2015; @elhija2012; @alorifi2008] additionally suggests the occurrence of a loss of emphasis in Lebanese, or more generally Levantine or Western dialects of Arabic, likely as a result of the contact with the Indo-European languages mentioned above.\n\nIt is against this background, and as part of efforts to document the precise place of secondary articulation of emphasis in Lebanese Arabic, as well as to document whether or not emphasis has, indeed, been lost in the variety, that the data used here [from @sakr2025] was collected. It consists of UTI recordings, by 5 participants, of CVb stimuli. The onset was either an emphatic or an unemphatic ('plain'), voiced or voiceless, alveolar, plosive or fricative /t, ṭ, d, ḍ, s, ṣ, z, ẓ/; when talking about a plain/emphatic pair, we denote them /T, D, S, Z/. The nucleus was one of five vowel qualities [see @sakr2019] present in Lebanese, which we will denote with /A, E, I, O, U/ to signal that these are neither to be taken as phonemes or exact phonetic realisations. The coda was the voiced bilabial plosive /b/.\n\nEach recording consisted of four stimuli in randomized order, covering forty syllables, in five repetitions; for a total of 1000 recordings. The subset of the data used here is from 35ms before consonant offset, defined as the burst for the plosives and as the end of the frication noise for the fricatives.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndlc_emph_f <- readRDS(\"data/sakr2025/dlc_emph_f.rds\")\n```\n:::\n\n\n\n\nSince the procedure to fit and plot MGAMs is the same as the one presented in @sec-gam-vc-coart, we won't be showing the code in this section, but readers can find the code in the Article Notebook, at <https://stefanocoretta.github.io/mv_uti/index-preview.html>.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(mgcv)\n\nfi <- \"data/cache/emph_gam.rds\"\n\nif (file.exists(fi)) {\n  emph_gam <- readRDS(fi)\n} else {\n  emph_gam <- gam(\n    list(\n      X_z ~ vow_emph +\n        s(knot, by = vow_emph, k = 5) +\n        s(knot, participant, by = vow_emph, bs = \"fs\", m = 1, k = 5),\n      Y_z ~ vow_emph +\n        s(knot, by = vow_emph, k = 5) +\n        s(knot, participant, by = vow_emph, bs = \"fs\", m = 1, k = 5)\n    ),\n    data = dlc_emph_f,\n    family = mvn(d = 2)\n  )\n  \n  saveRDS(emph_gam, fi)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nframe_emph <- expand_grid(\n  participant = unique(dlc_emph_f$participant),\n  vow_emph = unique(dlc_emph_f$vow_emph),\n  knot = seq(0, 10, by = 0.1)\n)\n\nexcl <- c(\n  \"s(Knot,participant):vow_emphA.Emphatic\",\n  \"s(Knot,participant):vow_emphE.Emphatic\",\n  \"s(Knot,participant):vow_emphI.Emphatic\",\n  \"s(Knot,participant):vow_emphO.Emphatic\",\n  \"s(Knot,participant):vow_emphU.Emphatic\",\n  \"s.1(Knot,participant):vow_emphA.Emphatic\",\n  \"s.1(Knot,participant):vow_emphE.Emphatic\",\n  \"s.1(Knot,participant):vow_emphI.Emphatic\",\n  \"s.1(Knot,participant):vow_emphO.Emphatic\",\n  \"s.1(Knot,participant):vow_emphU.Emphatic\",\n  \"s(Knot,participant):vow_emphA.Plain\",\n  \"s(Knot,participant):vow_emphE.Plain\",\n  \"s(Knot,participant):vow_emphI.Plain\",\n  \"s(Knot,participant):vow_emphO.Plain\",\n  \"s(Knot,participant):vow_emphU.Plain\",\n  \"s.1(Knot,participant):vow_emphA.Plain\",\n  \"s.1(Knot,participant):vow_emphE.Plain\",\n  \"s.1(Knot,participant):vow_emphI.Plain\",\n  \"s.1(Knot,participant):vow_emphO.Plain\",\n  \"s.1(Knot,participant):vow_emphU.Plain\"\n)\n\nemph_gam_p <- predict(emph_gam, frame_emph, se.fit = TRUE, exclude = excl) |>\n  as.data.frame() |>\n  as_tibble()\ncolnames(emph_gam_p) <- c(\"X\", \"Y\", \"X_se\", \"Y_se\")\n\nemph_gam_p <- bind_cols(frame_emph, emph_gam_p) |> \n  # pick any speaker, random effects have been removed\n  filter(participant == \"Sak\") |> \n  mutate(\n    X_lo = X - (1.96 * X_se),\n    X_hi = X + (1.96 * X_se),\n    Y_lo = Y - (1.96 * Y_se),\n    Y_hi = Y + (1.96 * Y_se)\n  ) |> \n  separate(vow_emph, c(\"vowel\", \"emph\"))\n```\n:::\n\n\n\n\n@fig-emph-ci shows the predicted tongue contours of emphatic and plain consonants, split by following vowel. First, the following vowel exercises an appreciable amount of coarticulation on the preceding consonant. The vowel-induced coarticulation seem to be modulating how the emphatic vs plain distinction is implemented (or not): in the context of the vowels /A, O, U/, emphatic consonants are produced with a retracted body and root, indicating pharyngealisation. On the other hand, in the context of the front vowels /E, I/, there is visibly less distinction between emphatic and plain consonants, which is virtually absent in /E/. However, when plotting the predictions for the different vocalic contexts and different speakers, the picture becomes more complex.\n\n\n\n\n::: {#cell-fig-emph-ci .cell}\n\n```{.r .cell-code .hidden}\nemph_gam_p |> \n  ggplot(aes(X, Y, colour = emph)) +\n  geom_errorbarh(aes(xmin = X_lo, xmax = X_hi), alpha = 0.25) +\n  geom_errorbar(aes(ymin = Y_lo, ymax = Y_hi), alpha = 0.25) +\n  geom_point(size = 1, alpha = 0.75) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  coord_fixed() +\n  facet_grid(cols = vars(vowel)) +\n  theme_light() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Predicted tongue contours with 95% CIs from an MGAM of Lebanese Arabic emphatic and plain coronal consonants.](index_files/figure-html/fig-emph-ci-1.png){#fig-emph-ci width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nemph_gam_p_2 <- predict(emph_gam, frame_emph, se.fit = TRUE) |>\n  as.data.frame() |>\n  as_tibble()\ncolnames(emph_gam_p_2) <- c(\"X\", \"Y\", \"X_se\", \"Y_se\")\n\nemph_gam_p_2 <- bind_cols(frame_emph, emph_gam_p_2) |>\n  mutate(\n    X_lo = X - (1.96 * X_se),\n    X_hi = X + (1.96 * X_se),\n    Y_lo = Y - (1.96 * Y_se),\n    Y_hi = Y + (1.96 * Y_se)\n  ) |> \n  separate(vow_emph, c(\"vowel\", \"emph\"))\n```\n:::\n\n\n\n\nIn @fig-emph-part, predictions have been calculated for individual speakers (see Article Notebook online, linked above, for the code). First, there is a good deal of individual variation: some speakers show a clear differentiation of the tongue shape in emphatic and plain consonants, while in other speakers the difference is less obvious. FAK produced emphatic and plain consonants with virtually the same tongue shape. Just to pick another example, BAR velarised rather than pharyngealised the emphatic consonants followed by /I/, while BAY pharyngealised them. Plotting predictions of individual speakers can reveal idiosyncratic patterns which are not visible when plotting overall predictions.\n\n\n\n\n::: {#cell-fig-emph-part .cell}\n\n```{.r .cell-code .hidden}\nemph_gam_p_2 |> \n  ggplot(aes(X, Y, colour = emph)) +\n  geom_errorbarh(aes(xmin = X_lo, xmax = X_hi), alpha = 0.5) +\n  geom_errorbar(aes(ymin = Y_lo, ymax = Y_hi), alpha = 0.5) +\n  facet_grid(rows = vars(participant), cols = vars(vowel)) +\n  coord_fixed()\n```\n\n::: {.cell-output-display}\n![Predicted tongue contours with 95% CIs from an MGAM of Lebanese Arabic emphatic and plain coronal consonants split by speaker.](index_files/figure-html/fig-emph-part-1.png){#fig-emph-part width=672}\n:::\n:::\n\n\n\n\n# Multivariate Functional Principal Component Analysis {#sec-fpca}\n\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique. For an introduction to PCA we recommend @kassambara2017a. Functional PCA (FPCA) is an extension of PCA: while classical PCA works by finding common variance in a set of variables (and by reducing the variables to Principal Components that explain that common variance), FPCA is a PCA applied to a functional representation of varying numerical variables [@gubian2019; @gubian2019a; @gubian2024]: a typical example is time-series data, with a variable changing over time. The trajectory of the time-varying variable is encoded into a function with a set of coefficients and the values of those coefficients are submit to PCA. When more than one time-varying variable is needed, this is where Multivariate FPCA (MFPCA) come in [@gubian2024].\n\nMFPCA is an FPCA applied to two or more varying variables. Note that the variable does not have to be *time*-varying. The variation can be on any linear variable: in the case of DLC-tracked UTI data, the variation happens along the knot number. Look back at @fig-tongue-xy: the two varying variables are the X and Y coordinates, which are varying along the DLC knots. As with MGAMs, it is these two varying trajectories that are submitted to MFPCA.\n\n## VC coarticulation\n\nWe will apply Multivariate Functional Principal Component Analysis (MFPCA) to the data introduced in @sec-gam-vc-coart. The following code has been adapted from @gubian2024. The packages below are needed to run MFPCA (except landmarkregUtils, they are available on CRAN).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(fda)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: splines\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: fds\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: rainbow\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: MASS\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'MASS'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: pcaPP\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: RCurl\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'RCurl'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:tidyr':\n\n    complete\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: deSolve\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'fda'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:graphics':\n\n    matplot\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:datasets':\n\n    gait\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(funData)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'funData'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:ggplot2':\n\n    ggplot\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:stats':\n\n    integrate\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(MFPCA)\n# install.packages(\"remotes\")\n# remotes::install_github(\"uasolo/landmarkregUtils\")\nlibrary(landmarkregUtils)\n```\n:::\n\n\n\n\nThe format required to work through MFPCA is a \"long\" format with one column containing the coordinate labels (*x* or *y* coordinate) and another with the coordinate values. We can easily pivot the data with `pivot_longer()`. Note that we are using the *z*-scored coordinate values (`X_z` and `Y_z`). If you are not unsure about what the code in this section, it is always useful to inspect intermediate and final output.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndlc_voff_long <- dlc_voff_f |> \n  # Select relevant columns\n  dplyr::select(X_z, Y_z, frame_id, knot, vowel, c2_place, language, speaker) |> \n  # Pivot data to longer format. Saves coordinate labels to column `dim`\n  pivot_longer(c(X_z, Y_z), names_to = \"dim\")\n```\n:::\n\n\n\n\nIn the second step, we create a `multiFunData` object: this is a special type of list object, with the observations of the two coordinates (`X_z` and `Y_z`) as two matrices of dimension $N \\cdot 11$, where $N$ is the number of tongue contours and $11$ is for the 11 knots returned by DLC. Three columns in the data are used to create the `multiFunData` object: one column with the id of each contour (in our data, `frame_id`), a time or series column (`knot`) and the column with the coordinate values (`value`).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ncurves_fun_2d <- lapply(\n  c(\"X_z\", \"Y_z\"),\n  function(y) {\n    long2irregFunData(\n      dlc_voff_long |> filter(dim == {{y}}),\n      # Tongue contour ID\n      id = \"frame_id\",\n      # Knot column\n      time = \"knot\",\n      # X/Y coordinate values\n      value = \"value\"\n    ) |> \n    as.funData()\n  }\n) |> \n  multiFunData()\n```\n:::\n\n\n\n\nOnce we have our `multFunData` object, we can use the `MFPCA()` function to compute an MFPCA. In this tutorial we will compute the first two PCs, but you can compute up to $K-1$ PCs where $K$ is the number of DLC knots in the data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Number of PC to compute\nn_pc <- 2\n\n# Compute MFPCA\nmfpca <- MFPCA(\n  curves_fun_2d,\n  M = n_pc,\n  uniExpansions = list(list(type = \"uFPCA\"), list(type = \"uFPCA\"))\n)\n```\n:::\n\n\n\n\nWe can quickly calculate the proportion of explained variance of each PC with the following code. PC1 and PC2 together explain almost 100% of the variance in our data. The higher the variance explained, the better the variance patterns in the data are captured.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Proportion of explained variance\nmfpca$values  / sum(mfpca$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7108713 0.2891287\n```\n\n\n:::\n:::\n\n\n\n\nThe best way to assess the effect of the PC scores on the shape of the tongue contours is to plot the predicted tongue contours based on a set of representative PC scores. In order to be able to plot the predicted contours, we need to calculate them from the MFPCA object. Gubian suggests plotting predicted curves at score intervals based on fractions of the scores standard deviation. This is what the following code does.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Get the PC score SD\nsd_fun <- sqrt(mfpca$values)\n\n# PC curves to be plotted\npc_curves <- expand_grid(\n  PC = 1:n_pc,\n  dim = 1:2,\n  # Set the SD fraction, from -1 SD to +1 SD, with increments by 0.25\n  sd_frac = seq(-1, 1, by = 0.25)\n) |>\n  group_by(PC, dim, sd_frac) |>\n  # We can now calculate the predicted contour with funData2long1().\n  # reframe() is needed because the funData2long1() function returns a data frame\n  # the has more rows than the original.\n  reframe(\n    funData2long1(\n      mfpca$meanFunction[[dim]] +\n        sd_frac * sd_fun[PC] * mfpca$functions[[dim]][PC],\n      time = \"knot\", value = \"value\"\n    )\n  ) |> \n  # We relabel the dimensions\n  mutate(\n    dim = factor(dim, levels = c(2, 1), labels = c('Y_z', 'X_z'))\n  )\n```\n:::\n\n\n\n\nThe created data frame `pc_curves` has the predicted values of the X and Y coordinates *along the knots*. This is the same structure as @fig-tongue-xy, with the knot number on the *x*-axis and the coordinates on the *y*-axis. Of course, what we are after is the X/Y plot of the tongue contours, rather than the knot/coordinate plot as needed to fit an MFPCA. For the sake of clarity, we first plot the predicted curves for X and Y separately. @fig-pc-curves shows these. The plot is composed of four panels: the top two are the predicted curves along knot number for the Y coordinates (based on PC1 in the left panel and PC2 in the right panel). Interpreting the effect of the PCs on the X and Y coordinates separately allows one to observe vertical (Y coordinate) and horizontal (X coordinate) differences in tongue position independently. However, note that the vector of muscle contractions in the tongue are not simply along a vertical/horizontal axis [@honda1996; @wrench2024]. Looking at a full tongue contour (in an X/Y coordinates plot) will generally prove to be more straightforward.\n\n\n\n\n::: {#cell-fig-pc-curves .cell}\n\n```{.r .cell-code .hidden}\npc_curves |> \n  ggplot(aes(\n    x = knot, y = value, group = sd_frac, color = sd_frac\n  )) +\n  geom_line() +\n  scale_color_gradient2(\n    low = \"#762a83\", mid = \"grey\", high = \"#1b7837\",\n    breaks = c(-1, 0 , 1)\n  ) +\n  facet_grid(\n    cols = vars(PC), rows = vars(dim),\n    scales = \"free_y\",\n    labeller = labeller(PC = ~str_glue(\"PC{.x}\"))\n  ) +\n  labs(color = expression(frac(s[k], sigma[k]))) +\n  geom_line(\n    data = pc_curves |> filter(sd_frac == 0),\n    color = 'black', linewidth = 1.2\n  )\n```\n\n::: {.cell-output-display}\n![Predicted curves along knot number for X and Y coordinates, as obtained from an MFPCA.](index_files/figure-html/fig-pc-curves-1.png){#fig-pc-curves width=672}\n:::\n:::\n\n\n\n\nIn order to plot tongue contours in the X/Y coordinate system, we simply need to pivot the data to a wider format.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\npc_curves_wide <- pc_curves |> \n  pivot_wider(names_from = dim)\n```\n:::\n\n\n\n\n@fig-contours plots the predicted contours based on the the PC scores (specifically, fractions of the standard deviation of the PC scores). The *x* and *y*-axes correspond to the X and Y coordinates of the tongue contour, with the effect of PC1 in the left panel and the effect of PC2 in the right panel. A higher PC1 score (green lines in the left panel) suggest a lowering of the tongue body/dorsum and raising of the tongue tip. Since the data contains velar and coronal consonants, we take this to be capturing the velar/coronal place of articulation effect. A higher PC2 score (green lines in the right panel) corresponds to an overall higher tongue position. Considering that the back/central vowels /a, o, u/ are included in this data set, we take PC2 to be related with the effect of vowel on the tongue shape at closure onset.\n\n\n\n\n::: {#cell-fig-contours .cell}\n\n```{.r .cell-code .hidden}\npc_curves_wide |> \n  ggplot(aes(x = X_z, y = Y_z, group = sd_frac, color = sd_frac)) +\n  geom_path() +\n  scale_color_gradient2(\n    low = \"#762a83\", mid = \"grey\", high = \"#1b7837\",\n    breaks = c(-1, 0 , 1)\n  ) +\n  facet_wrap(\n    vars(PC),\n    labeller = labeller(PC = ~str_glue(\"PC{.x}\"))\n  ) +\n  coord_fixed()\n```\n\n::: {.cell-output-display}\n![Predicted tongue contours as obtained from an MFPCA.](index_files/figure-html/fig-contours-1.png){#fig-contours width=672}\n:::\n:::\n\n\n\n\nGiven the patterns in @fig-contours, we can expect to see differences in PC2 scores based on the vowel if there is VC coarticulation. We can obtain the PC scores of each observation in the data with the following code.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\npc_scores <- mfpca$scores |>\n  `colnames<-`(paste0(\"PC\", 1:n_pc)) |>\n  as_tibble() |>\n  bind_cols(dlc_voff_long |> distinct(frame_id, vowel, c2_place, language))\n```\n:::\n\n\n\n\n@fig-pc-scores plots PC scores by language (rows), consonant place (columns) and vowel (colour). Both in Italian and Polish, we can observe a clear coarticulatory effect of /u/ on the production of coronal stops (and perhaps minor differences in /a/ vs /o/). On the other hand, the effect of vowel in velar stops seems to be minimal, again in both languages. This is not entirely surprising, since while coronal stops allow for adjustments of (and coarticulatory effect on) the tongue body, velar stops do not since it is precisely the tongue body/dorsum that is raised to produce the velar closure.\n\n\n\n\n::: {#cell-fig-pc-scores .cell}\n\n```{.r .cell-code .hidden}\npc_scores |> \n  filter(PC2 < 0.5) |>\n  ggplot(aes(x = PC1, y = PC2, color = vowel)) +\n  geom_point() +\n  stat_ellipse() +\n  facet_grid(cols = vars(c2_place), rows = vars(language)) +\n  scale_color_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![PC1/PC2 scores by language, consonant place of articulation and vowel.](index_files/figure-html/fig-pc-scores-1.png){#fig-pc-scores width=672}\n:::\n:::\n\n\n\n\nOnce one has established which patterns each PC is capturing, PC scores can be submitted to further statistical modelling, like for example regression models where the PC scores are outcome variables and several predictors are include to assess possible differences in PC scores.\n\n## Emphaticness\n\nIn this section we will run an MFPCA analysis on the Lebanese Arabic data. Since the procedure is the same as in the previous section, the code will not be shown here, but can be viewed in the Article Notebook, at <https://stefanocoretta.github.io/mv_uti/index-preview.html>.\n\n\n\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\ndlc_emph_long <- dlc_emph_f |> \n  pivot_longer(c(X_z, Y_z), names_to = \"dim\") |> \n  group_by(dim, participant) |> \n  mutate(\n    value = (value - mean(value)) / sd(value)\n  ) |> \n  ungroup()\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\ncurves_fun_2d <- lapply(\n  c(\"X_z\", \"Y_z\"),\n  function(y) {\n    long2irregFunData(\n      dlc_emph_long |> filter(dim == {{y}}),\n      # Tongue contour ID\n      id = \"frame_id\",\n      # Knot column\n      time = \"knot\",\n      # X/Y coordinate values\n      value = \"value\"\n    ) |> \n    as.funData()\n  }\n) |> \n  multiFunData()\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# Number of PC to compute\nn_pc <- 2\n\n# Compute MFPCA\nmfpca <- MFPCA(\n  curves_fun_2d,\n  M = n_pc,\n  uniExpansions = list(list(type = \"uFPCA\"), list(type = \"uFPCA\"))\n)\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# Proportion of explained variance\nmfpca$values  / sum(mfpca$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7309506 0.2690494\n```\n\n\n:::\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# Get the PC score SD\nsd_fun <- sqrt(mfpca$values)\n\n# PC curves to be plotted\npc_curves <- expand_grid(\n  PC = 1:n_pc,\n  dim = 1:2,\n  # Set the SD fraction, from -1 SD to +1 SD, with increments by 0.25\n  sd_frac = seq(-1, 1, by = 0.25)\n) |>\n  group_by(PC, dim, sd_frac) |>\n  # We can now calculate the predicted contour with funData2long1().\n  # reframe() is needed because the funData2long1() function returns a data frame\n  # the has more rows than the original.\n  reframe(\n    funData2long1(\n      mfpca$meanFunction[[dim]] +\n        sd_frac * sd_fun[PC] * mfpca$functions[[dim]][PC],\n      time = \"knot\", value = \"value\"\n    )\n  ) |> \n  # We relabel the dimensions\n  mutate(\n    dim = factor(dim, levels = c(2,1), labels = c('Y_z', 'X_z'))\n  )\n```\n:::\n\n::: {#cell-fig-emph-pc-curves .cell .hidden}\n\n```{.r .cell-code .hidden}\npc_curves |> \n  ggplot(aes(\n    x = knot, y = value, group = sd_frac, color = sd_frac\n  )) +\n  geom_line() +\n  scale_color_gradient2(\n    low = \"#762a83\", mid = \"grey\", high = \"#1b7837\",\n    breaks = c(-1, 0 , 1)\n  ) +\n  facet_grid(\n    cols = vars(PC), rows = vars(dim),\n    scales = \"free_y\",\n    labeller = labeller(PC = ~str_glue(\"PC{.x}\"))\n  ) +\n  labs(color = expression(frac(s[k], sigma[k]))) +\n  geom_line(\n    data = pc_curves |> filter(sd_frac == 0),\n    color = 'black', linewidth = 1.2\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-emph-pc-curves-1.png){#fig-emph-pc-curves width=672}\n:::\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\npc_curves_wide <- pc_curves |> \n  pivot_wider(names_from = dim)\n```\n:::\n\n\n\n\n@fig-emph-curves-wide illustrates the reconstructed tongue contours (taken from 35 ms before the CV boundary) in Lebanese Arabic, based on the MFPCA. PC1 captures the low-back/high-front diagonal movement. PC2, on the other hand, seems to be restricted to high/low movement at the back of the oral cavity. Emphatic consonants, if produced with a constricted pharynx (i.e. pharyngealised), should have a lower PC1. If on the other hand they are produced with a raised tongue dorsum (i.e. velarised), they should have a lower PC2 (lower PC scores are in purple in @fig-emph-curves-wide).\n\n\n\n\n::: {#cell-fig-emph-curves-wide .cell}\n\n```{.r .cell-code .hidden}\npc_curves_wide |> \n  ggplot(aes(x = X_z, y = Y_z, group = sd_frac, color = sd_frac)) +\n  geom_path() +\n  scale_color_gradient2(\n    low = \"#762a83\", mid = \"grey\", high = \"#1b7837\",\n    breaks = c(-1, 0 , 1)\n  ) +\n  facet_wrap(\n    vars(PC),\n    labeller = labeller(PC = ~str_glue(\"PC{.x}\"))\n  ) +\n  coord_fixed()\n```\n\n::: {.cell-output-display}\n![Predicted tongue contours of Lebanese Arabic coronal consonants as obtained from an MFPCA.](index_files/figure-html/fig-emph-curves-wide-1.png){#fig-emph-curves-wide width=672}\n:::\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\npc_scores <- mfpca$scores |>\n  `colnames<-`( paste0(\"PC\", 1:n_pc)) |>\n  as_tibble() |>\n  bind_cols(dlc_emph_long |> distinct(frame_id, emph, vowel, participant))\n```\n:::\n\n\n\n\n@fig-emph-speakers plots the PC scores for each vowel, emphaticness and speaker combination. Points are coloured based on emphaticness: emphatic in green and plain in orange. This figure illustrates well how the PC scores can capture individual variation: some speakers show clear separation of emphatic and plain tokens, while others do not. In most cases, PC1 is doing the heavy lifting of distinguishing emphatic and plain: recall that PC1 captures the front-high/back-low diagonal; a low PC1 indicates tongue dorsum and root backing, in other words pharyngealisation. Indeed, PC1 tends to be lower in emphatic tokens in several speakers, like Bar, Bay, Mro and Sak, especially with the vowels /A, O, U/. On the other hand, Bar's emphatic and plain tokens for vowels /E, I/ do not show a PC1 difference, but rather a PC2 difference: PC2 captures tongue dorsum/body raising, hence indicating velarisation. It is possible that in Bar's productions of emphatic consonants followed by /E, I/ the distinction with plain is produced by velarisation, compared to the pharyngealisation of emphatic consonants followed by /A, O, U/. Velarisation, rather than pharyngealisation, in the /E, I/ contexts makes sense given that the tongue root has to be front in the production of those vowels.\n\n\n\n\n::: {#cell-fig-emph-speakers .cell}\n\n```{.r .cell-code .hidden}\npc_scores |> \n  ggplot(aes(x = PC1, y = PC2, colour = emph, label = vowel)) +\n  geom_point(alpha = 0.5) +\n  scale_color_brewer(palette = \"Dark2\") +\n  stat_ellipse() +\n  facet_grid(cols = vars(participant), rows = vars(vowel))\n```\n\n::: {.cell-output-display}\n![PC1 and PC2 scores by vowel, consonant type and speaker.](index_files/figure-html/fig-emph-speakers-1.png){#fig-emph-speakers width=672}\n:::\n:::\n\n\n\n\n@fig-emph-pc1 and @fig-emph-pc2 illustrate one way to plot the PC scores individually for PC1 and PC2. We won't include here a full description of the plots, since they should be self-explanatory, but we flag to the reader that these type of plots can be helpful in illustrating specific patterns in PC1 or PC2.\n\n\n\n\n::: {#cell-fig-emph-pc1 .cell}\n\n```{.r .cell-code .hidden}\npc_scores |> \n  ggplot(aes(vowel, PC1, colour = emph)) +\n  geom_jitter(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.25) +\n  scale_color_brewer(palette = \"Dark2\") +\n    facet_wrap(vars(participant))\n```\n\n::: {.cell-output-display}\n![PC1 scores of emphatic and plain consonants by speaker and vowel.](index_files/figure-html/fig-emph-pc1-1.png){#fig-emph-pc1 width=672}\n:::\n:::\n\n::: {#cell-fig-emph-pc2 .cell}\n\n```{.r .cell-code .hidden}\npc_scores |> \n  ggplot(aes(vowel, PC2, colour = emph)) +\n  geom_jitter(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.25) +\n  scale_color_brewer(palette = \"Dark2\") +\n    facet_wrap(vars(participant))\n```\n\n::: {.cell-output-display}\n![PC2 scores of emphatic and plain consonants by speaker and vowel.](index_files/figure-html/fig-emph-pc2-1.png){#fig-emph-pc2 width=672}\n:::\n:::\n\n\n\n\nFinally, it will usually be helpful to reconstruct the predicted tongue contours of specific context. For example, we might be interested in showing the average tongue contours for emphatic and plain consonant followed by each of the five vowels in the data. This is shown in @fig-vow-emph. In order to obtain the reconstructed contours, we first need to calculate mean PC scores for each vowel. This can be done through the following code. We recommend to inspect the `pc_scores_mean` object.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npc_scores_mean <- pc_scores |>\n  # Group by variables based on which we want to obtain mean values.\n  group_by(vowel, emph) |> \n  # Sumarise data to obtain means\n  summarise(\n    PC1 = mean(PC1),\n    PC2 = mean(PC2),\n    .groups = \"drop\"\n  ) |> \n  # Add \"dimensions\", i.e. X and Y coordinates\n  mutate(\n    dim = list(c(1, 2))\n  ) |> \n  # Unnes the dim column\n  unnest(dim)\n```\n:::\n\n\n\n\nThe following code calculates the reconstructed tongue contours based on both PC1 and PC2. One could also calculate the reconstructed contours at factions of the standard deviation of the scores if one wished so, like it was done above.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npc_curves_2 <- pc_scores_mean |> \n  group_by(dim, vowel, emph) |>\n  # We can now calculate the predicted contour with funData2long1().\n  # reframe() is needed because the funData2long1() function returns a data frame\n  # the has more rows than the original.\n  reframe(\n    funData2long1(\n      mfpca$meanFunction[[dim]] +\n        # We add PC1\n        PC1 * mfpca$functions[[dim]][1] +\n        # and we add PC2 as well\n        PC2 * mfpca$functions[[dim]][2],\n      time = \"knot\", value = \"value\"\n    )\n  ) |> \n  # We relabel the dimensions\n  mutate(\n    dim = factor(dim, levels = c(2,1), labels = c('Y_z', 'X_z'))\n  ) |> \n  pivot_wider(names_from = dim, values_from = value)\n```\n:::\n\n\n\n\nFinally, @fig-vow-emph plots the reconstructed contours. Based on this figure, we do find pharyngealisation in emphatic consonants followed by /A, O, U/ on average, while pharyngealisation is absent in the context of /E, I/.\n\n\n\n\n::: {#cell-fig-vow-emph .cell}\n\n```{.r .cell-code .hidden}\npc_curves_2 |> \n  ggplot(aes(X_z, Y_z, colour = emph)) +\n  geom_path() +\n  facet_wrap(vars(vowel))\n```\n\n::: {.cell-output-display}\n![Reconstructed tongue contours based on PC1/PC2.](index_files/figure-html/fig-vow-emph-1.png){#fig-vow-emph width=672}\n:::\n:::\n\n\n\n\n# Advantages and disadvantages {#sec-procons}\n\nBoth Multivariate GAMs and FPCA are a useful way to model DLC-tracked ultrasound tongue imaging data. However, each possesses advantages and disadvantages.\n\nMultivariate GAMs can model tongue contours in specific contexts and combinations thereof, like different vowels, consonant, prosodic contexts and so on. The rather complex model structure required to fit multivariate GAMs to tongue data comes at a computational cost and interpretative cost. Computationally, multivariate GAMs can take hours to estimate even the most simple models. Interpretationally, comparing different tongue contours quantitatively based on the output of a multivariate GAM is non-trivial, given that the tongue contour is in fact a curve reconstructed from the smooths of the X and Y coordinates along knot (in other words, the model does not model tongue contours directly). Moreover, there is no straightforward way to use traditional methods to assess (frequentist) statistical significance. From a practical point of view, a multivariate GAM ends up being a mathematically complex way of obtaining a sort of average tongue contour.\n\nMultivariate FPCA, on the other hand, are computationally efficient. Even with very large data sets, the computation of Principal Components is relatively quick. Moreover, the obtained PCs can be interpreted straightforwardly by plotting the effect of changing the PC score on the reconstructed tongue contour (as we did for example in @fig-contours). One possible disadvantage of multivariate FPCA is that it is usually not known what type of variation each obtained PC captures. This is illustrated in the two case studies in @sec-fpca. In the VC coarticulation data, PC1 corresponded to the coronal/velar difference in consonants, while PC2 to the difference in vowel. In the emphaticness data, PC1 captured the low-back/high-front diagonal movement, while PC2 to the high/low movement at the back of the oral cavity. In other words, until one has run the MPFCA, one does not know what PC will correspond to which axis of differences and whether the PCs will capture relevant difference at all (it can happen that the variation one is after is so minimal relative to other, more substantial cases of variation, that it will not be captures at all). It is possible that qualitatively homogeneous data sets might return PCs that have the same or very similar interpretation, but this has not been systematically tested [perhaps with the exception of simple PCA run on vowel formant data, which suggests that the first two PCs capture the two diagonals, @faber1995; @hoole1999; @strycharczuk2021; @strycharczuk2025 which make sense in light of the tongue position and shape model expounded by @honda1996].\n\nAnother advantage of MFPCA is that, provided that the PCs have captured relevant characteristics, the PCs can be submitted to further modelling using regression with the inclusion of relevant predictors (like different categorical variables of interest). We have not done so in this tutorial to keep the scope and length of the tutorial manageable, but both case studies presented in @sec-fpca are amenable to such follow-up analysis.\n\nBased on the advantages and disadvantages of each of multivariate GAMs and FPCA, we suggest researchers to use MFPCA as the preferred and default approach to analyse DLC-tracked tongue contour data and to resort to multivariate GAMs if MFPCA fails to capture relevant variation.\n\n# Conclusions\n\nTBA\n\n# References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}